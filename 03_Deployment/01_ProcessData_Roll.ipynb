{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALLEST_DS_SIZE = 100\n",
    "ROLLING_STEPS = 100\n",
    "\n",
    "# Get data file names\n",
    "START_T_COUNT = 0\n",
    "START_F_COUNT = 500\n",
    "\n",
    "SPLIT_T_RATIO = 0.7\n",
    "\n",
    "NUM_NO_FEAT_COLUMN = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential,load_model\n",
    "from tensorflow.keras.layers import Dense,Dropout,BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.base import BaseEstimator,TransformerMixin\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "\n",
    "tf.random.set_seed(20) # to fix the randomization\n",
    "np.random.seed(20) # to fix the randomizationin sklearn\n",
    "sklearn.random.seed(1)\n",
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'D:\\_1_Technicals\\DataScience_Bootcamp\\Final_project\\Flutter_ArduinoConnect\\00_DataSets'\n",
    "filenames = glob.glob(path + \"/**/*.csv\")\n",
    "\n",
    "# shuffeling the files as its always ordered alphabetic\n",
    "\n",
    "shuffled_filenames = random.sample(filenames, len(filenames))\n",
    "\n",
    "dfs = []\n",
    "num_t = START_T_COUNT\n",
    "num_f = START_F_COUNT\n",
    "for filename in shuffled_filenames:\n",
    "   \n",
    "    df = pd.read_csv(filename, index_col=False)\n",
    "    if df.shape[0] > SMALLEST_DS_SIZE:\n",
    "        df.columns= [\"time\",\"rotR_x\",\"rotR_y\",\"rotR_z\",\"acc_x\",\"acc_y\",\"acc_z\",\"or_x\",\"or_y\",\"or_z\",\"grav_x\",\"grav_y\",\"grav_z\"]\n",
    "              \n",
    "        _, tail = os.path.split(filename)\n",
    "        df[\"action\"] = str(tail).split(\".\")[0]\n",
    "        \n",
    "\n",
    "        if df.action.str.startswith(\"t_\").sum():\n",
    "            df[\"label\"] = 1\n",
    "            df[\"action_num\"] = num_t # 0 - 499\n",
    "            num_t += 1\n",
    "        else:\n",
    "            df[\"label\"] = 0\n",
    "            df[\"action_num\"] = num_f # 500 - 1000\n",
    "            num_f += 1\n",
    "        \n",
    "        df = df.drop(columns=\"time\")\n",
    "        dfs.append(df)\n",
    "        \n",
    "df_all = pd.concat(dfs,axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data (Train, Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_t_ds = len(df_all.query(\"action_num < @START_F_COUNT\").action.unique())\n",
    "num_f_ds = len(df_all.query(\"action_num >= @START_F_COUNT\").action.unique())\n",
    "\n",
    "num_train_t = round(num_t_ds * SPLIT_T_RATIO)\n",
    "num_train_f = round(num_f_ds * SPLIT_T_RATIO)\n",
    "\n",
    "train_df = df_all[(df_all.action_num.between(START_T_COUNT,START_T_COUNT+num_train_t,inclusive=\"both\"))|\n",
    "                (df_all.action_num.between(START_F_COUNT,START_F_COUNT+num_train_f,inclusive=\"both\"))]\n",
    "\n",
    "test_df = df_all[(df_all.action_num.between(START_T_COUNT+num_train_t,START_F_COUNT,inclusive=\"neither\"))|\n",
    "                (df_all.action_num > START_F_COUNT+num_train_f)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.iloc[:,:(-1*NUM_NO_FEAT_COLUMN)]\n",
    "y_train = train_df.label\n",
    "\n",
    "X_test = test_df.iloc[:,:(-1*NUM_NO_FEAT_COLUMN)]\n",
    "y_test = test_df.label\n",
    "\n",
    "X_train, val_X, y_train, val_y = train_test_split(X_train, y_train, shuffle=True,test_size=0.2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create preprocess Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes df of size roll_steps+1, divide data, \n",
    "\n",
    "class PreprocessDataTransformer(BaseEstimator, TransformerMixin,auto_wrap_output_keys=None):\n",
    "    def __init__(self, *, roll_steps= 100, state=\"train\"):\n",
    "        self.roll_steps = roll_steps\n",
    "        self.state = state\n",
    "        #super().__init__()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_ = X.copy()\n",
    "        try:\n",
    "            X_.columns= [\"time\",\"rotR_x\",\"rotR_y\",\"rotR_z\",\"acc_x\",\"acc_y\",\"acc_z\",\"or_x\",\"or_y\",\"or_z\",\"grav_x\",\"grav_y\",\"grav_z\"]\n",
    "            X_.acc_x = X_.acc_x/4096.0\n",
    "            X_.acc_y = X_.acc_y/4096.0\n",
    "            X_.acc_z = X_.acc_z/4096.0\n",
    "            \n",
    "            X_.rotR_x = X_.rotR_x/(16.4*100)\n",
    "            X_.rotR_y = X_.rotR_y/(16.4*100)\n",
    "            X_.rotR_z = X_.rotR_z/(16.4*100)\n",
    "            \n",
    "            X_.grav_x = X_.grav_x/4096.0\n",
    "            X_.grav_y = X_.grav_y/4096.0\n",
    "            X_.grav_z = X_.grav_z/4096.0\n",
    "            \n",
    "            X_.or_x = X_.or_x/100\n",
    "            X_.or_y = X_.or_y/100\n",
    "            X_.or_z = X_.or_z/100\n",
    "            X_ = X_.drop(columns=\"time\")\n",
    "        except Exception as e:\n",
    "            print(\" Normalization failed: \", e)\n",
    "        \n",
    "        try:\n",
    "            X_.insert(0, 'accel_x', X_.acc_x + X_.grav_x)\n",
    "            X_.insert(1, 'accel_y', X_.acc_y + X_.grav_y)\n",
    "            X_.insert(2, 'accel_z', X_.acc_z + X_.grav_z)\n",
    "            X_.insert(3, 'accel_norm', np.sqrt(X_.accel_x**2 + X_.accel_y**2 + X_.accel_z**2))\n",
    "            X_ = X_.drop(['accel_x', 'accel_y', 'accel_z'], axis=1)\n",
    "        except Exception as e:\n",
    "            print(\" Basic Features failed: \", e)\n",
    "    \n",
    "        try:\n",
    "            j = 1\n",
    "            for i in X_.columns:\n",
    "                X_.insert(j, f'{i}_rmean', X_[i].rolling(self.roll_steps).mean())\n",
    "                X_.insert(j+1, f'{i}_rstd', X_[i].rolling(self.roll_steps).std())\n",
    "                X_.insert(j+2, f'{i}_rmed', X_[i].rolling(self.roll_steps).median())\n",
    "                j += 4 \n",
    "                \n",
    "            if self.state == \"train\":\n",
    "                # Dropping all rows where the lag overlapped two different subjects/trials.\n",
    "                for i in range(self.roll_steps):\n",
    "                    X_ = X_.drop([i])\n",
    "            else:\n",
    "                X_ = X_.iloc[-1]  # If deployment, take last part\n",
    "                X_ = np.asarray(X_).reshape(1,-1)\n",
    "        except Exception as e:\n",
    "            print(\" Rolling failed: \", e)\n",
    "            \n",
    "        if type(X_) != np.ndarray :\n",
    "            X_ = np.asarray(X_)\n",
    "        return X_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_prepocessor = PreprocessDataTransformer(roll_steps= 100, state=\"train\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "cnn = Sequential()\n",
    "cnn.add(Dense(100, input_dim=input_dim, activation='relu'))\n",
    "cnn.add(Dropout(rate =0.2)) # drop some of the neurals in the back prop. analysis to avoid overfitting\n",
    "cnn.add(BatchNormalization())\n",
    "cnn.add(Dense(40, activation='relu'))\n",
    "cnn.add(Dropout(rate =0.5))\n",
    "cnn.add(BatchNormalization())\n",
    "cnn.add(Dense(20, activation='relu'))\n",
    "cnn.add(Dropout(rate =0.4))\n",
    "cnn.add(BatchNormalization())\n",
    "cnn.add(Dense(1, activation='sigmoid'))\n",
    "opt = keras.optimizers.Adam(learning_rate=0.005)\n",
    "cnn.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=0, patience=10)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn.fit(X_train, y_train, epochs=100, batch_size=250,callbacks=[es,mc]\n",
    "#                   , validation_data=(val_X,val_y)\n",
    "#                   )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe = make_pipeline(my_prepocessor,cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Normalization failed:  Length mismatch: Expected axis has 12 elements, new values have 13 elements\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 25221\n  y sizes: 33201\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pipe\u001b[39m.\u001b[39;49mfit(X_train, y_train, sequential__epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, sequential__batch_size\u001b[39m=\u001b[39;49m\u001b[39m250\u001b[39;49m,sequential__callbacks\u001b[39m=\u001b[39;49m[es,mc], sequential__validation_data\u001b[39m=\u001b[39;49m(val_X,val_y))\n",
      "File \u001b[1;32mc:\\Users\\Jacqueline\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\pipeline.py:405\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    403\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    404\u001b[0m         fit_params_last_step \u001b[39m=\u001b[39m fit_params_steps[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[1;32m--> 405\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator\u001b[39m.\u001b[39mfit(Xt, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_last_step)\n\u001b[0;32m    407\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Jacqueline\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Jacqueline\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\data_adapter.py:1851\u001b[0m, in \u001b[0;36m_check_data_cardinality\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1844\u001b[0m     msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m  \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m sizes: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1845\u001b[0m         label,\n\u001b[0;32m   1846\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\n\u001b[0;32m   1847\u001b[0m             \u001b[39mstr\u001b[39m(i\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(single_data)\n\u001b[0;32m   1848\u001b[0m         ),\n\u001b[0;32m   1849\u001b[0m     )\n\u001b[0;32m   1850\u001b[0m msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mMake sure all arrays contain the same number of samples.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1851\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 25221\n  y sizes: 33201\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "pipe.fit(X_train, y_train, sequential__epochs=100, sequential__batch_size=250,sequential__callbacks=[es,mc], sequential__validation_data=(val_X,val_y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_saved_model = load_model('best_model.h5')\n",
    "_, train_acc = best_saved_model.evaluate(X_train, y_train, verbose=0)\n",
    "_, test_acc = best_saved_model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Train: %.2f, Test: %.2f' % (train_acc*100, test_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_model_predictions = best_saved_model.predict(X_test)\n",
    "mae = round(mean_absolute_error(y_test, best_model_predictions),3)\n",
    "print(\"mean absolute error is\",mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert predicted label rom analog to binary value\n",
    "bin_best_model_predictions=[]\n",
    "for val in best_model_predictions:\n",
    "    if val >=0.5:\n",
    "        bin_best_model_predictions.append(1)\n",
    "    else:\n",
    "        bin_best_model_predictions.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "conf_mat = confusion_matrix(bin_best_model_predictions, y_test_roll)\n",
    "conf_mat_disp = ConfusionMatrixDisplay(confusion_matrix=conf_mat)\n",
    "conf_mat_disp.plot(cmap='Greens')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
