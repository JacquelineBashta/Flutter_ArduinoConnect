{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "import common_functions as cf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_pickle(\"merged_w_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering (basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fe = cf.fe_basic_features(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fe.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting(Test/train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df,test_df = cf.data_split_TrainTest(df_fe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape,test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess (Normalizing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "last_feat_column = df_fe.columns[-4]\n",
    "\n",
    "for act_num in train_df.action_num.unique():\n",
    "    std = StandardScaler().set_output(transform='pandas')\n",
    "    \n",
    "    train_df_w_act_num = train_df.loc[train_df.action_num == act_num,:last_feat_column].copy()\n",
    "    std.fit(train_df_w_act_num)\n",
    "    train_df.loc[train_df.action_num == act_num,:last_feat_column] = std.transform(train_df_w_act_num)\n",
    "\n",
    "\n",
    "for act_num in test_df.action_num.unique():\n",
    "    std = StandardScaler().set_output(transform='pandas')\n",
    "    \n",
    "    test_df_w_act_num = test_df.loc[test_df.action_num == act_num,:last_feat_column].copy()\n",
    "    std.fit(test_df_w_act_num)\n",
    "    test_df.loc[test_df.action_num == act_num,:last_feat_column] = std.transform(test_df_w_act_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape,test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data (Feature/Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    X_train,y_train = cf.data_split_FeatLabel(train_df)\n",
    "    X_test,y_test = cf.data_split_FeatLabel(test_df)\n",
    "   \n",
    "    X_train = np.asarray(X_train).reshape(int(X_train.shape[0]/cf.SEQUENCE_SIZE) , cf.SEQUENCE_SIZE, X_train.shape[1])\n",
    "    y_train = np.asarray(y_train).reshape(int(y_train.shape[0]/cf.SEQUENCE_SIZE) , cf.SEQUENCE_SIZE)\n",
    "    y_train_compact=[]\n",
    "    for lis in y_train:\n",
    "        if sum(lis) == 0:\n",
    "            y_train_compact.append(0)\n",
    "        elif sum(lis) == cf.SEQUENCE_SIZE:\n",
    "            y_train_compact.append(1)\n",
    "        else:\n",
    "            print(\"something wrong\")\n",
    "    y_train_compact = np.asarray(y_train_compact)\n",
    "    X_test = np.asarray(X_test).reshape(int(X_test.shape[0]/cf.SEQUENCE_SIZE) , cf.SEQUENCE_SIZE, X_test.shape[1])\n",
    "    y_test = np.asarray(y_test).reshape(int(y_test.shape[0]/cf.SEQUENCE_SIZE) , cf.SEQUENCE_SIZE)\n",
    "    y_test_compact=[]\n",
    "    for lis in y_test:\n",
    "        if sum(lis) == 0:\n",
    "            y_test_compact.append(0)\n",
    "        elif sum(lis) == cf.SEQUENCE_SIZE:\n",
    "            y_test_compact.append(1)\n",
    "        else:\n",
    "            print(\"something wrong\")\n",
    "    y_test_compact = np.asarray(y_test_compact)\n",
    "\n",
    "    return  X_train,y_train_compact,X_test,y_test_compact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "trainX, trainy, testX, testy = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX.shape,trainy.shape, testX.shape, testy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = trainX.shape[2]\n",
    "\n",
    "# reshape data into time steps of sub-sequences\n",
    "n_steps = 2\n",
    "n_length = int(cf.SEQUENCE_SIZE/n_steps) \n",
    "\n",
    "trainX_resh = trainX.reshape((trainX.shape[0], n_steps, n_length, n_features))\n",
    "testX_resh = testX.reshape((testX.shape[0], n_steps, n_length, n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainy.shape, testy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX_resh.shape,testX_resh.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neurel Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential,load_model\n",
    "from tensorflow.keras.layers import Dense,Flatten,Dropout,LSTM,TimeDistributed,MaxPooling1D,Conv1D\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "tf.random.set_seed(20) # to fix the randomization\n",
    "np.random.seed(20)# to fix the randomizationin sklearn\n",
    "sklearn.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(TimeDistributed(Conv1D(filters=10, kernel_size=3, activation='relu'), input_shape=(None,n_length,n_features)))\n",
    "model.add(TimeDistributed(Conv1D(filters=10, kernel_size=3, activation='relu')))\n",
    "model.add(TimeDistributed(Dropout(0.2)))\n",
    "model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.Adam(learning_rate=0.02)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "X_train, val_X, y_train, val_y = train_test_split(trainX_resh, trainy, shuffle=True,test_size=0.2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, val_X.shape, y_train.shape, val_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "mc = ModelCheckpoint('best_model_LSTM.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "# fit network\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=250,validation_data=(val_X,val_y),callbacks=[es,mc],verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.loc[:, ['loss',\"val_loss\"]].plot()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_saved_model = load_model('best_model_LSTM.h5')\n",
    "_, train_acc = best_saved_model.evaluate(trainX_resh, trainy, verbose=0)\n",
    "_, test_acc = best_saved_model.evaluate(testX_resh, testy, verbose=0)\n",
    "print('Train: %.2f, Test: %.2f' % (train_acc*100, test_acc*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
